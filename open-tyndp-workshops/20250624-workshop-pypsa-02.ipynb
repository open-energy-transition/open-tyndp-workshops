{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1bee99052b4b9a7",
   "metadata": {},
   "source": [
    "# Workshop 2: Introduction to `Snakemake` workflows, update on new Open-TYNDP features & benchmarking framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6b79e8",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "If you have not yet set up Python on your computer, you can execute this tutorial in your browser via [Google Colab](https://colab.research.google.com/). Click on the rocket in the top right corner and launch \"Colab\". If that doesn't work download the `.ipynb` file and import it in [Google Colab](https://colab.research.google.com/).\n",
    "\n",
    "Then install the following packages by executing the following command in a Jupyter cell at the top of the notebook.\n",
    "\n",
    "```sh\n",
    "!pip install pypsa atlite pandas geopandas xarray matplotlib hvplot geoviews plotly highspy holoviews folium mapclassify snakemake\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "id": "60eb9731",
   "metadata": {},
   "source": [
    "# uncomment for running this notebook on Colab\n",
    "# !pip install pypsa atlite pandas geopandas xarray matplotlib hvplot geoviews plotly highspy holoviews folium mapclassify snakemake"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "29301c27",
   "metadata": {},
   "source": [
    "# import packages\n",
    "from IPython.display import Code, SVG, Image, display\n",
    "from urllib.request import urlretrieve\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import cartopy.crs as ccrs\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pypsa\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pypsa.plot.maps.static import (\n",
    "    add_legend_circles,\n",
    "    add_legend_patches,\n",
    "    add_legend_lines,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e1ba5ec3",
   "metadata": {},
   "source": [
    "urls = {\n",
    "    \"data/data_raw.csv\": \"https://storage.googleapis.com/open-tyndp-data-store/workshop-02/data_raw.csv\",\n",
    "    \"data/open-tyndp.zip\": \"https://storage.googleapis.com/open-tyndp-data-store/workshop-02/open-tyndp.zip\",\n",
    "    \"data/network_NT_presolve_highres_2030.nc\": \"https://storage.googleapis.com/open-tyndp-data-store/workshop-02/network_NT_presolve_highres_2030.nc\",\n",
    "}\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "for name, url in urls.items():\n",
    "    if os.path.exists(name):\n",
    "        print(f\"File {name} already exists. Skipping download.\")\n",
    "    else:\n",
    "        print(f\"Retrieving {name} from GCP storage.\")\n",
    "        urlretrieve(url, name)\n",
    "        print(f\"File available in {name}.\")\n",
    "\n",
    "to_dir=\"data/open-tyndp\"\n",
    "if not os.path.exists(to_dir):\n",
    "    with zipfile.ZipFile(\"data/open-tyndp.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(to_dir)\n",
    "print(f\"Open-TYNDP available in '{to_dir}'.\")\n",
    "\n",
    "print(\"Done\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f68125e0502973f",
   "metadata": {},
   "source": [
    "# The `Snakemake` tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44c6959896526ad",
   "metadata": {},
   "source": [
    "<img src=\"snakemake_logo.png\" width=\"300px\" />\n",
    "\n",
    "The `Snakemake` workflow management system is a tool to create reproducible and scalable data analyses.\n",
    "Workflows are described via a human readable, Python based language. They can be seamlessly scaled to server, cluster, grid and cloud environments, without the need to modify the workflow definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f61ff44",
   "metadata": {},
   "source": [
    "Snakemake follows the [GNU Make](https://www.gnu.org/software/make) paradigm: workflows are defined in terms of so-called `rules` that define how to create a set of output files from a set of input files. Dependencies between the rules are determined automatically, creating a DAG (directed acyclic graph) of jobs that can be automatically parallelized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c5c5e23a63e560",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "Documentation for this package is available at https://snakemake.readthedocs.io/. You can also check out a [slide deck Snakemake Tutorial](https://slides.com/johanneskoester/snakemake-tutorial) by Johannes Köster (2024).\n",
    "\n",
    "Mölder, F., Jablonski, K.P., Letcher, B., Hall, M.B., Tomkins-Tinch, C.H., Sochat, V., Forster, J., Lee, S., Twardziok, S.O., Kanitz, A., Wilm, A., Holtgrewe, M., Rahmann, S., Nahnsen, S., Köster, J., 2021. Sustainable data analysis with Snakemake. F1000Res 10, 33.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e82c9e22d6b8ce6",
   "metadata": {},
   "source": [
    "## A minimal Snakemake example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f936ab",
   "metadata": {},
   "source": [
    "To check out how this looks in practice, we've prepared a minimal Snakemake example workflow that processes some data. The minimal workflow consists of the following rules:\n",
    "- `retrieve_data`\n",
    "- `build_data`\n",
    "- `prepare_network`\n",
    "- `solve_network`\n",
    "- `plot_benchmark`\n",
    "- `all`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c33282d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"minimal_workflow.png\" width=\"400px\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534d9440",
   "metadata": {},
   "source": [
    "We will first need to load the raw data file used in this minimal example into our google drive:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458220db",
   "metadata": {},
   "source": [
    "### The `Snakefile` and `rules`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ef510f",
   "metadata": {},
   "source": [
    "The rules need to be defined in a so-called `Snakefile` that sits in the same directory as your current working directory. For our minimal example the `Snakefile` looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "id": "c9dfe4ae",
   "metadata": {},
   "source": [
    "Code(filename='Snakefile', language=\"Python\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "677e4c85",
   "metadata": {},
   "source": [
    "### Calling a workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7ebade",
   "metadata": {},
   "source": [
    "You can then execute the workflow by asking for the target file `data/benchmark.pdf` or any intermediate file:\n",
    "```\n",
    "snakemake -call data/benchmark.pdf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9edee26",
   "metadata": {},
   "source": [
    "Alternatively you can also execute the workflow by calling a rule that produces an intermediate file:\n",
    "```\n",
    "snakemake -call build_data\n",
    "```\n",
    "NOTE: It is important that you cannot call a rule that includes a wildcard without specifying what the wildcard should be filled with. Otherwise Snakemake will not know what to propagate back."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6235bc82",
   "metadata": {},
   "source": [
    "Or you can call the common rule `all` which can be used to execute the entire workflow. It takes the final workflow output as its input and thus requires all previous dependent rules to be run as well:\n",
    "```\n",
    "snakemake -call all\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fd843e",
   "metadata": {},
   "source": [
    "A very important instrument is the `-n` flag which executes a `dry-run`. It is recommended to always first execute a `dry-run` before the actual execution of the workflow. This simply prints out the directed acyclic graph (DAG) of the workflow to investigate without actually executing it.\n",
    "\n",
    "Let's try this out and investigate the output:"
   ]
  },
  {
   "cell_type": "code",
   "id": "ec82288e",
   "metadata": {},
   "source": [
    "! snakemake -call all -n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dd6ef812",
   "metadata": {},
   "source": [
    "### Visualizing the `DAG` of a worflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff53df2",
   "metadata": {},
   "source": [
    "You can also visualize the `DAG` of jobs using the `--dag` flag and the Graphviz `dot` command. This will not run the workflow but only create the visualization:\n",
    "```\n",
    "snakemake -call all --dag | dot -Tsvg > dag.svg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "id": "e051153b",
   "metadata": {},
   "source": [
    "! snakemake -call all --dag | sed -n \"/digraph/,\\$p\" | dot -Tpng > dag_minimal.png"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1a4af4831b0ac3b4",
   "metadata": {},
   "source": [
    "display(Image('dag_minimal.png'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "99494ce7",
   "metadata": {},
   "source": [
    "Alternatively, you can also visualize a filegraph like the figure above which includes also some information about the inputs and outputs to each of the rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d70368",
   "metadata": {},
   "source": [
    "You can reproduce the figure from above with the following command:\n",
    "```\n",
    "snakemake -call all --filegraph | dot -Tsvg > filegraph.svg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "id": "c1ec22d6",
   "metadata": {},
   "source": [
    "! snakemake -call all --filegraph | sed -n \"/digraph/,\\$p\" | dot -Tsvg > filegraph_minimal.svg"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "72c99998a2f27173",
   "metadata": {},
   "source": [
    "display(SVG('filegraph_minimal.svg'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cc74c230",
   "metadata": {},
   "source": [
    "## Task 1: Executing a workflow with Snakemake\n",
    "\n",
    "a) For our minimal example, execute a `dry-run` to produce the intermediate file `data/base_2030.nc`.\n",
    "\n",
    "b) Execute the workflow and investigate what happens if you try to execute the workflow again.\n",
    "\n",
    "c) Delete the final output files `data/benchmark.pdf` and investigate what happens if you try to execute the workflow again.\n",
    "\n",
    "d) Import the raw input data file `data/data_raw.csv` using pandas and save it again overwriting the original file. Investigate what happens if you try to execute the workflow again. <br>\n",
    "Hint: Alternative you can also just `touch` the file by executing `from pathlib import Path` and `Path(\"data/data_raw.csv\").touch()`\n",
    "\n",
    "e ) Finally, open the `Snakefile` and add a second rule that processes the file `data_raw_2.csv` using the same script as the `build_data` rule. Add the output of this new rule as a second input to the `prepare_network` rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fc1a64ecb68ac3",
   "metadata": {},
   "source": [
    "## Using Snakemake to launch the open-TYNDP workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759df1ccb8f8f179",
   "metadata": {},
   "source": [
    "We have already retrieved a prebuilt version of the `open-tyndp` GitHub repository into our working directory..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b5b10c",
   "metadata": {},
   "source": [
    "The `open-tyndp` contains the following structure (directories which might be of particular interest to you are marked in bold):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea24d976",
   "metadata": {},
   "source": [
    "- **benchmarks**: will store snakemake benchmarks (does not exist initially)\n",
    "- **config**: configurations used in the study\n",
    "- cutouts: will store raw weather data cutouts from atlite (does not exist initially)\n",
    "- **data**: includes input data that is not produced by any snakemake rule. Various different input files are retrieved from external storage and stored in this directory\n",
    "- doc: includes all files necessary to build the readthedocs documentation of PyPSA-Eur\n",
    "- **envs**: includes all the mamba environment specifications to run the workflow\n",
    "- logs: will store log files (does not exist initially)\n",
    "- **notebooks**: includes all the notebooks used for ad-hoc analysis\n",
    "- report: contains all files necessary to build the report; plots and result files are generated automatically\n",
    "- **rules**: includes all the snakemake rules loaded in the Snakefile\n",
    "- **resources**: will store intermediate results of the workflow which can be picked up again by subsequent rules (does not exist initially)\n",
    "- **results**: will store the solved PyPSA network data, summary files and plots (does not exist initially)\n",
    "- **scripts**: includes all the Python scripts executed by the snakemake rules to build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29b1200c02ce786",
   "metadata": {},
   "source": [
    "We now need to change our working directory to this new directory:"
   ]
  },
  {
   "cell_type": "code",
   "id": "f190d685184837b4",
   "metadata": {},
   "source": [
    "os.chdir('data/open-tyndp')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "50e39aa4",
   "metadata": {},
   "source": [
    "Let's check that we are indeed in the new directory now:"
   ]
  },
  {
   "cell_type": "code",
   "id": "4db99e626b8b0309",
   "metadata": {},
   "source": [
    "os.getcwd()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ac7e3e50",
   "metadata": {},
   "source": [
    "We can now use Snakemake to call some of the rules to produce outputs with the `open-tyndp` PyPSA model. \n",
    "\n",
    "We will use the prepared TYNDP configuration file (`config/config.tyndp.yaml`) and schedule a dry-run with `-n` as we only want to investigate the DAG of the workflow:"
   ]
  },
  {
   "cell_type": "code",
   "id": "a547c6f4a2d82607",
   "metadata": {},
   "source": [
    "! snakemake -call all --configfile config/config.tyndp.yaml -n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c9208a03",
   "metadata": {},
   "source": [
    "The corresponding rule graph to this workflow will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "id": "33f4913b",
   "metadata": {},
   "source": [
    "! snakemake -call all -F --rulegraph | sed -n \"/digraph/,\\$p\" | dot -Tpng > rulegraph_open_tyndp.png"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4749ee23",
   "metadata": {},
   "source": [
    "display(Image(\"rulegraph_open_tyndp.png\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d009cc9b",
   "metadata": {},
   "source": [
    "As you can see this workflow is much more complicated than our minimal example from the beginning.\n",
    "\n",
    "However, the general idea remains the same. We retrieve data wich we consequently process, then we prepare the model network and we solve it before we postprocess the results (summary, plotting, benchmarks)."
   ]
  },
  {
   "cell_type": "code",
   "id": "dbca229b",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2b4aaf85",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "If you are executing this notebook on your local machine, you can also use the `conda` package manager to install the `open-tyndp` environment and run the workflow instead of dry-runs:\n",
    "```\n",
    "conda env create --file envs/<YourSystemOS>-pinned.yaml\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0bcb21af782ebd",
   "metadata": {},
   "source": [
    "## Task 2: Adjusting the Open-TYNDP workflow with the configuration file\n",
    "\n",
    "a) Make some changes in the configuration file and call another **dry-run** of the `open-tyndp` model again to see the changes to the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753af7b6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "240f5e7d",
   "metadata": {},
   "source": [
    "# Update on new features"
   ]
  },
  {
   "cell_type": "code",
   "id": "19d279af",
   "metadata": {},
   "source": [
    "# For the purposes of this workshop we will primarily focus on the National Trends (NT) scenario"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "97193552",
   "metadata": {},
   "source": [
    "## Time Series"
   ]
  },
  {
   "cell_type": "code",
   "id": "25b8565e",
   "metadata": {},
   "source": [
    "# read network\n",
    "# --- Electricity demand\n",
    "# --- PECD\n",
    "# look for renewables components\n",
    "# Explain / remind difference between time varying and fixed attributes, how to access them\n",
    "# Plot both time of parameters\n",
    "# Compare to Report"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b60f0668",
   "metadata": {},
   "source": [
    "First, we will import the solved network for the National Trends (NT) scenario for 2030 and 2040"
   ]
  },
  {
   "cell_type": "code",
   "id": "f2079eb8",
   "metadata": {},
   "source": [
    "# hourly networks\n",
    "n_2030_NT_presolve_highres = pypsa.Network(\"../network_NT_presolve_highres_2030.nc\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3333d214",
   "metadata": {},
   "source": [
    "### Electricity demand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b52e94",
   "metadata": {},
   "source": [
    "We can then explore the electricity demand that is attached to the network. Can you remember how to access `Loads` timeseries in PyPSA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46775be5",
   "metadata": {},
   "source": [
    "Correct! You can use the `loads_t` key and its `p_set` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "id": "95d3ae39",
   "metadata": {},
   "source": [
    "loads_2030 = n_2030_NT_presolve_highres.loads_t.p_set\n",
    "display(loads_2030)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "05458163",
   "metadata": {},
   "source": [
    "Let's plot these electricity demand time series:"
   ]
  },
  {
   "cell_type": "code",
   "id": "767187f7",
   "metadata": {},
   "source": [
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Plot each load time series\n",
    "for load in loads_2030.columns:\n",
    "    ax.plot(loads_2030.index, loads_2030[load], label=load, linewidth=1.5, alpha=0.8)\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlabel(\"Time\", fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel(\"Load [MW]\", fontsize=12, fontweight='bold')\n",
    "ax.set_title(\"Electricity Load Time Series National Trends 2030\", fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Grid\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# Legend\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', \n",
    "          frameon=True, shadow=True, fontsize=9, ncols=6)\n",
    "plt.tight_layout()\n",
    "plt.xticks(rotation=45, ha='right');"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5cdd854e",
   "metadata": {},
   "source": [
    "This is very confusing to look at. Let's filter that down a bit..."
   ]
  },
  {
   "cell_type": "code",
   "id": "7f730872",
   "metadata": {},
   "source": [
    "# group country profiles together\n",
    "country_mapping = n_2030_NT_presolve_highres.buses.query(\"carrier=='AC'\").country\n",
    "loads_2030_by_country = (\n",
    "    n_2030_NT_presolve_highres\n",
    "    .loads_t\n",
    "    .p_set.T\n",
    "    .rename(country_mapping, axis=0)\n",
    "    .groupby(\"Load\").sum().T\n",
    "    .query(\"index  >= '2009-03-01' and index <= '2009-03-07'\")\n",
    ")\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Plot each load time series\n",
    "for load in loads_2030_by_country.columns:\n",
    "    ax.plot(loads_2030_by_country.index, loads_2030_by_country[load], label=load, linewidth=1.5, alpha=0.8)\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlabel(\"Time\", fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel(\"Load [MW]\", fontsize=12, fontweight='bold')\n",
    "ax.set_title(\"Electricity Load Time Series National Trends 2030 - March 1-7\", fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Grid\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# Legend\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', \n",
    "          frameon=True, shadow=True, fontsize=9, ncols=6)\n",
    "plt.tight_layout()\n",
    "plt.xticks(rotation=45, ha='right');"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "90e32485",
   "metadata": {},
   "source": [
    "#### Task: Can you remember how to use the **PyPSA Statistics** module that we introduced in the last workshop to interactively visualize these electricity demand inputs from the network:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7672932",
   "metadata": {},
   "source": [
    "First we need to load a lower resolution network that was solved, so we can use the statistics module to analyse it."
   ]
  },
  {
   "cell_type": "code",
   "id": "02b9c885",
   "metadata": {},
   "source": [
    "n_2030_NT_solved_lowres = pypsa.Network(\"results/tyndp/NT/networks/base_s_all___2030.nc\")\n",
    "n_2040_NT_solved_lowres = pypsa.Network(\"results/tyndp/NT/networks/base_s_all___2040.nc\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "96f6a2eb",
   "metadata": {},
   "source": [
    "# let's fill missing colors first\n",
    "n_2030_NT_solved_lowres.carriers.loc[\"none\", \"color\"] = \"#000000\"\n",
    "n_2030_NT_solved_lowres.carriers.loc[\"\", \"color\"] = \"#000000\"\n",
    "# and define a helper variable\n",
    "s = n_2030_NT_solved_lowres.statistics"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4952e155",
   "metadata": {},
   "source": [
    "s.energy_balance(bus_carrier=\"low voltage\", comps=\"Load\", aggregate_time=False, groupby=False).T"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f9a4a6c0",
   "metadata": {},
   "source": [
    "fig, ax, facet_col  = s.withdrawal.plot.area(\n",
    "    bus_carrier=\"low voltage\",\n",
    "    y=\"value\",\n",
    "    x=\"snapshot\",\n",
    "    color=\"carrier\",\n",
    "    stacked=True,\n",
    "    facet_row=\"country\",\n",
    "    query=\"carrier == 'electricity' and country in ['DE', 'FR']\",\n",
    ")\n",
    "fig.suptitle('Electricity demand time series', y=1.05)\n",
    "ax[0,0].set_ylabel(\"Load [MW]\")\n",
    "ax[1,0].set_ylabel(\"Load [MW]\")\n",
    "ax[1,0].set_xlabel(\"Time\");"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "41247bcd",
   "metadata": {},
   "source": [
    "We can also look at all the countries at the same time..."
   ]
  },
  {
   "cell_type": "code",
   "id": "957d953b",
   "metadata": {},
   "source": [
    "fig, ax, facet_col  = s.withdrawal.plot.line(\n",
    "    bus_carrier=\"low voltage\",\n",
    "    y=\"value\",\n",
    "    x=\"snapshot\",\n",
    "    color=\"country\",\n",
    ")\n",
    "fig.suptitle('Electricity Load Time Series National Trends 2030', y=1.05)\n",
    "ax.set_ylabel(\"Load [MW]\")\n",
    "ax.set_xlabel(\"Time\");"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f3eff42",
   "metadata": {},
   "source": [
    "# Task: Take a few minutes to investigate the electricity demand series with the pypsa.statistics module"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b93a51e6",
   "metadata": {},
   "source": [
    "### PECD capacity factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acd585d",
   "metadata": {},
   "source": [
    "The Pan European Climate Database (PECD) provides capacity factor profiles for all the different renewable technologies used in the TYNDP. We processed these input data files into a Python and PyPSA friendly input format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea2437f",
   "metadata": {},
   "source": [
    "Let's start by looking at the processed capacity factor time series for solar PV Utility for 2030"
   ]
  },
  {
   "cell_type": "code",
   "id": "db551850",
   "metadata": {},
   "source": [
    "cf_pv_util = pd.read_csv(\"resources/tyndp/NT/pecd_data_LFSolarPVRooftop_2030.csv\", index_col=0, parse_dates=True)\n",
    "display(cf_pv_util.head(10))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f73476ae",
   "metadata": {},
   "source": [
    "For one week, these capacity factors look like this:"
   ]
  },
  {
   "cell_type": "code",
   "id": "71b438ae",
   "metadata": {},
   "source": [
    "loads_2030_by_country = (\n",
    "    cf_pv_util\n",
    "    .query(\"index  >= '2009-03-01' and index <= '2009-03-07'\")\n",
    ")\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Plot each load time series\n",
    "for load in loads_2030_by_country.columns:\n",
    "    ax.plot(loads_2030_by_country.index, loads_2030_by_country[load], label=load, linewidth=1.5, alpha=0.8)\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlabel(\"Time\", fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel(\"Capacity Factor\", fontsize=12, fontweight='bold')\n",
    "ax.set_title(\"Capacity Factor Time Series National Trends 2030 - March 1-7\", fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Grid\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# Legend\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', \n",
    "          frameon=True, shadow=True, fontsize=9, ncols=6)\n",
    "plt.tight_layout()\n",
    "plt.xticks(rotation=45, ha='right');"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d79330dc",
   "metadata": {},
   "source": [
    "# Task: In the open-tyndp, offshore wind, onshore wind and solar RES are added to the model at this time. \n",
    "# Take some time to investigate the network and find the corresponding capacity factor time series.\n",
    "# Hint: Capacity factors are implemented in PyPSA by using the per unit dispatch limit (`p_min_pu`)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aad8ea68",
   "metadata": {},
   "source": [
    "## Onshore wind and solar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4066e",
   "metadata": {},
   "source": [
    "The TYNDP provides expansion trajectories for given investment candidates and expandable technologies. For the implemented Onshore wind and solar technologies, this has been included within this beta release v0.3 of the Open-TYNDP model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4d750f",
   "metadata": {},
   "source": [
    "It is possible to retrieve those values from the network. However, for simplicity reasons, we will import the values from the processed input files for the `DE` scenario directly to investigate the entire trajectory paths at once."
   ]
  },
  {
   "cell_type": "code",
   "id": "a651700d",
   "metadata": {},
   "source": [
    "trajectories = pd.read_csv(\"resources/tyndp/DE/tyndp_trajectories.csv\")\n",
    "trajectories"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "28cbceb0",
   "metadata": {},
   "source": [
    "Similar to the capacity factor time series, we want to focus on the Solar PV Utility technology and their trajectory path. Let's take Germany (DE00) to investigate"
   ]
  },
  {
   "cell_type": "code",
   "id": "19b984f5",
   "metadata": {},
   "source": [
    "trajectories_pv_utility_de = (\n",
    "    trajectories\n",
    "    .query(\"carrier == 'solar-pv-utility' and bus == 'DE00'\")\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0e647262",
   "metadata": {},
   "source": [
    "def plot_trajectories(trajectory_df, title=\"Trajectories\"):\n",
    "    \"\"\"\n",
    "    Plot trajectories provided in DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set seaborn style\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "    # Single scenario\n",
    "    trajectory_df = trajectory_df.sort_values('pyear')\n",
    "\n",
    "    # Plot upper bound\n",
    "    ax.plot(trajectory_df['pyear'], trajectory_df['p_nom_max'].div(1e3),\n",
    "            label='Maximum Capacity (p_nom_max)', linewidth=2.5,\n",
    "            linestyle='--', color='#E63946', marker='o')\n",
    "\n",
    "    # Plot lower bound\n",
    "    ax.plot(trajectory_df['pyear'], trajectory_df['p_nom_min'].div(1e3),\n",
    "            label='Minimum Capacity (p_nom_min)', linewidth=2.5,\n",
    "            linestyle='--', color='#1D3557', marker='o')\n",
    "\n",
    "    # Fill between bounds\n",
    "    ax.fill_between(trajectory_df['pyear'],\n",
    "                    trajectory_df['p_nom_min'].div(1e3),\n",
    "                    trajectory_df['p_nom_max'].div(1e3),\n",
    "                    alpha=0.25, color='#457B9D', label='Trajectory Range')\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_xlabel(\"Planning Year\", fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel(\"Nominal Capacity [GW]\", fontsize=13, fontweight='bold')\n",
    "    ax.set_title(title,\n",
    "                 fontsize=15, fontweight='bold', pad=20)\n",
    "\n",
    "    # Legend\n",
    "    ax.legend(loc='best', frameon=True, shadow=True, fontsize=11)\n",
    "\n",
    "    # Grid\n",
    "    ax.grid(True, alpha=0.4, linestyle='-', linewidth=0.5)\n",
    "\n",
    "    # Format x-axis to show years as integers\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(5))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b9edd00fd760825f",
   "metadata": {},
   "source": [
    "plot_trajectories(trajectories_pv_utility_de, title=\"Solar PV Utility Capacity Trajectories DE scenario - DE00\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8d957d3c",
   "metadata": {},
   "source": [
    "aNow, let's access the network of the DE scenario to compare one of these values for 2040."
   ]
  },
  {
   "cell_type": "code",
   "id": "d35de90e",
   "metadata": {},
   "source": [
    "# TODO: include DE scenarios in open-tyndp.zip\n",
    "n_2040_DE_solved_lowres = pypsa.Network(\"results/tyndp/DE/networks/base_s_all___2040.nc\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "567331da",
   "metadata": {},
   "source": [
    "trajectories_pv_utility_de_from_network = (\n",
    "    n_2040_DE_solved_lowres\n",
    "    .generators\n",
    "    .query(\"carrier == 'solar-pv-utility' and bus == 'DE00'\")\n",
    "    [[\"p_nom\", \"p_nom_min\", \"p_nom_max\"]]\n",
    "    .div(1e3)  # in GW\n",
    ")\n",
    "trajectories_pv_utility_de_from_network"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "72a3c693",
   "metadata": {},
   "source": [
    "As we can see, the `p_nom_max` and `p_nom_min` values for 2040 do not match directly with the reported trajectories value analysed above. This is because each new Generator will have set trajectories that correspond to the new cummulatively installed capacities taking into account optimization results from previous years. So if we add up the existing capacity (`p_nom`) from 2030 and `p_nom_max` and `p_nom_min` from 2040, we will find the reported trajectory values from before again:"
   ]
  },
  {
   "cell_type": "code",
   "id": "e0899d25",
   "metadata": {},
   "source": [
    "trajectories_pv_utility_de_from_network.loc[\"DE00 0 solar-pv-utility-2040\", [\"p_nom_min\", \"p_nom_max\"]] + trajectories_pv_utility_de_from_network.loc[\"DE00 0 solar-pv-utility-2030\", \"p_nom\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "49f0d930",
   "metadata": {},
   "source": [
    "# Task: Reproduce this exercise for Onshore Wind. You can copy and reuse the existing code used above."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ffa610e3",
   "metadata": {},
   "source": [
    "## Offshore Hubs"
   ]
  },
  {
   "cell_type": "code",
   "id": "39fc5a2d",
   "metadata": {},
   "source": [
    "# read network\n",
    "# plot OH hubs maps\n",
    "# Constraitns\n",
    "# capacities\n",
    "# questions"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7ee8acb4",
   "metadata": {},
   "source": [
    "## H2 imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "9c32b1f8",
   "metadata": {},
   "source": [
    "# read network\n",
    "# plot H2 map with import corridors\n",
    "# table to showcase the corridors\n",
    "# optional: Task to investigate values"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8bac7c7f",
   "metadata": {},
   "source": [
    "There have also been some important additions to H2 infrastructure since our last workshop. The different H2 import corridors are now included in the model with a simple pipeline transport model representation such as for H2 reference grid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a5ac92",
   "metadata": {},
   "source": [
    "We can investigate our National Trends network from before to find a similar plot we showed last time. Let's define some (quite extensive) plotting functions from the open-tyndp workflow:"
   ]
  },
  {
   "cell_type": "code",
   "id": "ac2e5d0f",
   "metadata": {},
   "source": [
    "def group_import_corridors(df):\n",
    "    \"\"\"\n",
    "    Group pipes which connect same buses and return overall capacity.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # there are pipes for each investment period rename to AC buses name for plotting\n",
    "    df[\"index_orig\"] = df.index\n",
    "    df.rename(index=lambda x: x.split(\" - \")[0], inplace=True)\n",
    "    return df.groupby(level=0).agg(\n",
    "        {\"p_nom\": \"sum\", \"p_nom_opt\": \"sum\", \"index_orig\": \"first\"}\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_h2_map_base(\n",
    "    network, \n",
    "    map_opts, \n",
    "    proj,\n",
    "    figsize=(12, 12), \n",
    "    expanded=False, \n",
    "    regions_for_storage=None,     \n",
    "    color_h2_pipe = \"#499a9c\",\n",
    "    color_h2_imports = \"#FFA500\",\n",
    "    color_h2_node = \"#ff29d9\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots the base hydrogen network pipelines capacities, hydrogen buses and import potentials.\n",
    "    If expanded is enabled, the optimal capacities are plotted instead.\n",
    "    If regions are given, hydrogen storage capacities are plotted for those regions with aggregated H2 tank storage\n",
    "    and underground H2 cavern capacities.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network : pypsa.Network\n",
    "        PyPSA network for plotting the hydrogen grid. Can be either presolving or post solving.\n",
    "    map_opts : dict\n",
    "        Map options for plotting.\n",
    "    expanded : bool, optional\n",
    "        Whether to plot expanded capacities. Defaults to plotting only base network (p_nom).\n",
    "    regions_for_storage : gpd.GeoDataframe, optional\n",
    "        Geodataframe of regions to use for plotting hydrogen storage capacities. Index needs to match storage locations.\n",
    "        If none is given, no hydrogen storage capacities are plotted.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Saves the map plot as figure.\n",
    "    \"\"\"\n",
    "    n = network.copy()\n",
    "\n",
    "    linewidth_factor = 4e3\n",
    "\n",
    "    n.links.drop(\n",
    "        n.links.index[\n",
    "            ~(\n",
    "                n.links.carrier.str.contains(\"H2 pipeline\")\n",
    "                | n.links.carrier.str.contains(\"H2 import\")\n",
    "            )\n",
    "        ],\n",
    "        inplace=True,\n",
    "    )\n",
    "    n.links.drop(\n",
    "        n.links.index[n.links.carrier.str.contains(\"OH\")],\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    p_nom = \"p_nom_opt\" if expanded else \"p_nom\"\n",
    "    # capacity of pipes and imports\n",
    "    h2_pipes = n.links[n.links.carrier == \"H2 pipeline\"][p_nom]\n",
    "    h2_imports = n.links[n.links.carrier.str.contains(\"H2 import\")]\n",
    "\n",
    "    # group high and low import corridors together\n",
    "    h2_imports = group_import_corridors(h2_imports)[p_nom]\n",
    "    n.links.rename(index=lambda x: x.split(\" - \")[0], inplace=True)\n",
    "    # group links by summing up p_nom values and taking the first value of the rest of the columns\n",
    "    other_cols = dict.fromkeys(n.links.columns.drop([\"p_nom_opt\", \"p_nom\"]), \"first\")\n",
    "    n.links = n.links.groupby(level=0).agg(\n",
    "        {\"p_nom_opt\": \"sum\", \"p_nom\": \"sum\", **other_cols}\n",
    "    )\n",
    "\n",
    "    # set link widths\n",
    "    link_widths_pipes = h2_pipes / linewidth_factor\n",
    "    link_widths_imports = h2_imports / linewidth_factor\n",
    "    if link_widths_pipes.notnull().empty:\n",
    "        print(\"No base H2 pipeline capacities to plot.\")\n",
    "        return\n",
    "    link_widths_pipes = link_widths_pipes.reindex(n.links.index).fillna(0.0)\n",
    "    link_widths_imports = link_widths_imports.reindex(n.links.index).fillna(0.0)\n",
    "\n",
    "    # drop non H2 buses\n",
    "    n.buses.drop(\n",
    "        n.buses.index[\n",
    "            (~n.buses.carrier.str.contains(\"H2\")) | (n.buses.carrier.str.contains(\"OH\"))\n",
    "        ],\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # optionally add hydrogen storage capacities onto the map\n",
    "    if regions_for_storage is not None:\n",
    "        h2_storage = n.stores.query(\"carrier.str.contains('H2')\")\n",
    "        regions_for_storage[\"H2\"] = (\n",
    "            h2_storage.rename(index=h2_storage.bus.map(n.buses.location))\n",
    "            .e_nom_opt.groupby(level=0)\n",
    "            .sum()\n",
    "            .div(1e6)\n",
    "        )  # TWh\n",
    "        regions_for_storage[\"H2\"] = regions_for_storage[\"H2\"].where(\n",
    "            regions_for_storage[\"H2\"] > 0.1\n",
    "        )\n",
    "\n",
    "    # plot H2 pipeline capacities and imports\n",
    "    print(\"Plotting base H2 pipeline and import capacities.\")\n",
    "    fig, ax = plt.subplots(figsize=figsize, subplot_kw={\"projection\": proj})\n",
    "\n",
    "    n.plot.map(\n",
    "        geomap=True,\n",
    "        bus_sizes=0.1,\n",
    "        bus_colors=color_h2_node,\n",
    "        link_colors=color_h2_pipe,\n",
    "        link_widths=link_widths_pipes,\n",
    "        branch_components=[\"Link\"],\n",
    "        ax=ax,\n",
    "        **map_opts,\n",
    "    )\n",
    "\n",
    "    if regions_for_storage is not None:\n",
    "        regions_for_storage = regions_for_storage.to_crs(proj.proj4_init)\n",
    "        regions_for_storage.plot(\n",
    "            ax=ax,\n",
    "            column=\"H2\",\n",
    "            cmap=\"Blues\",\n",
    "            linewidths=0,\n",
    "            legend=True,\n",
    "            vmax=6,\n",
    "            vmin=0,\n",
    "            legend_kwds={\n",
    "                \"label\": \"Hydrogen Storage [TWh]\",\n",
    "                \"shrink\": 0.7,\n",
    "                \"extend\": \"max\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "    if not h2_imports.empty:\n",
    "        n.plot.map(\n",
    "            geomap=True,\n",
    "            bus_sizes=0,\n",
    "            link_colors=color_h2_imports,\n",
    "            link_widths=link_widths_imports,\n",
    "            branch_components=[\"Link\"],\n",
    "            ax=ax,\n",
    "            **map_opts,\n",
    "        )\n",
    "\n",
    "    sizes = [30, 10]\n",
    "    labels = [f\"{s} GW\" for s in sizes]\n",
    "    scale = 1e3 / linewidth_factor\n",
    "    sizes = [s * scale for s in sizes]\n",
    "\n",
    "    legend_kw = dict(\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(0.32, 1.13),\n",
    "        frameon=False,\n",
    "        ncol=1,\n",
    "        labelspacing=0.8,\n",
    "        handletextpad=1,\n",
    "    )\n",
    "\n",
    "    add_legend_lines(\n",
    "        ax,\n",
    "        sizes,\n",
    "        labels,\n",
    "        patch_kw=dict(color=\"lightgrey\"),\n",
    "        legend_kw=legend_kw,\n",
    "    )\n",
    "\n",
    "    legend_kw = dict(\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(0.55, 1.13),\n",
    "        labelspacing=0.8,\n",
    "        handletextpad=0,\n",
    "        frameon=False,\n",
    "    )\n",
    "\n",
    "    add_legend_circles(\n",
    "        ax,\n",
    "        sizes=[0.2],\n",
    "        labels=[\"H2 Node\"],\n",
    "        srid=n.srid,\n",
    "        patch_kw=dict(facecolor=color_h2_node),\n",
    "        legend_kw=legend_kw,\n",
    "    )\n",
    "\n",
    "    colors = (\n",
    "        [color_h2_pipe, color_h2_imports] if not h2_imports.empty else [color_h2_pipe]\n",
    "    )\n",
    "    labels = [\"H2 Pipeline\", \"H2 import\"] if not h2_imports.empty else [\"H2 Pipeline\"]\n",
    "\n",
    "    legend_kw = dict(\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(0, 1.13),\n",
    "        ncol=1,\n",
    "        frameon=False,\n",
    "    )\n",
    "\n",
    "    add_legend_patches(ax, colors, labels, legend_kw=legend_kw)\n",
    "\n",
    "    ax.set_facecolor(\"white\")\n",
    "\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aa3ec517",
   "metadata": {},
   "source": [
    "And plot the H2 reference grid together with the import corridors:"
   ]
  },
  {
   "cell_type": "code",
   "id": "8020a5da",
   "metadata": {},
   "source": [
    "n = n_2030_NT_solved_lowres.copy()\n",
    "\n",
    "def load_projection(plotting_params):\n",
    "    proj_kwargs = plotting_params.get(\"projection\", dict(name=\"EqualEarth\"))\n",
    "    proj_func = getattr(ccrs, proj_kwargs.pop(\"name\"))\n",
    "    return proj_func(**proj_kwargs)\n",
    "\n",
    "\n",
    "proj = load_projection(dict(name=\"EqualEarth\"))\n",
    "\n",
    "map_opts = {\n",
    "    \"boundaries\": [-11, 30, 34, 71],\n",
    "    \"geomap_colors\": {\n",
    "        \"ocean\": \"white\",\n",
    "        \"land\": \"white\",\n",
    "    },\n",
    "}\n",
    "\n",
    "if n.buses.country.isin([\"MA\", \"DZ\"]).any():\n",
    "    map_opts[\"boundaries\"] = list(np.add(map_opts[\"boundaries\"], [0, 0, -6, 0]))\n",
    "\n",
    "plot_h2_map_base(n, map_opts, proj)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f16ae2fe",
   "metadata": {},
   "source": [
    "# Benchmarking framework"
   ]
  },
  {
   "cell_type": "code",
   "id": "82e62f77",
   "metadata": {},
   "source": [
    "# Present metrics used (incl. reference to methodology)\n",
    "# Data sources used for comparison\n",
    "# Mention introduction of onwind and solar\n",
    "# Showcase current status\n",
    "# --- Table\n",
    "# --- Graphs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cc6f22d8",
   "metadata": {},
   "source": [
    "# Wrap up"
   ]
  },
  {
   "cell_type": "code",
   "id": "da40d5af",
   "metadata": {},
   "source": [
    "# Collect feeback via Slido"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f49df495",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-tyndp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
