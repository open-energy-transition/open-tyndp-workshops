{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1bee99052b4b9a7",
   "metadata": {},
   "source": [
    "# Workshop 2: Introduction to `Snakemake`, new features update & benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6b79e8",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "If you have not yet set up Python on your computer, you can execute this tutorial in your browser via [Google Colab](https://colab.research.google.com/). Click on the rocket in the top right corner and launch \"Colab\". If that doesn't work download the `.ipynb` file and import it in [Google Colab](https://colab.research.google.com/).\n",
    "\n",
    "Then install the following packages by executing the following command in a Jupyter cell at the top of the notebook.\n",
    "\n",
    "```sh\n",
    "!pip install pypsa atlite pandas geopandas xarray matplotlib hvplot geoviews plotly highspy holoviews folium mapclassify snakemake\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eb9731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment for running this notebook on Colab\n",
    "# !pip install pypsa atlite pandas geopandas xarray matplotlib hvplot geoviews plotly highspy holoviews folium mapclassify snakemake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29301c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from IPython.display import Code, SVG, Image, display\n",
    "from urllib.request import urlretrieve\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import matplotlib.dates as mdates\n",
    "import cartopy.crs as ccrs\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pypsa\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pypsa.plot.maps.static import (\n",
    "    add_legend_circles,\n",
    "    add_legend_patches,\n",
    "    add_legend_lines,\n",
    ")\n",
    "\n",
    "pypsa.options.params.statistics.round = 3\n",
    "pypsa.options.params.statistics.drop_zero = True\n",
    "pypsa.options.params.statistics.nice_names = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ba5ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = {\n",
    "    \"data/data_raw.csv\": \"https://storage.googleapis.com/open-tyndp-data-store/workshop-02/data_raw.csv\",\n",
    "    \"data/open-tyndp.zip\": \"https://storage.googleapis.com/open-tyndp-data-store/workshop-02/open-tyndp.zip\",\n",
    "    \"data/network_NT_presolve_highres_2030.nc\": \"https://storage.googleapis.com/open-tyndp-data-store/workshop-02/network_NT_presolve_highres_2030.nc\",\n",
    "}\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "for name, url in urls.items():\n",
    "    if os.path.exists(name):\n",
    "        print(f\"File {name} already exists. Skipping download.\")\n",
    "    else:\n",
    "        print(f\"Retrieving {name} from GCP storage.\")\n",
    "        urlretrieve(url, name)\n",
    "        print(f\"File available in {name}.\")\n",
    "\n",
    "to_dir = \"data/open-tyndp\"\n",
    "if not os.path.exists(to_dir):\n",
    "    with zipfile.ZipFile(\"data/open-tyndp.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(to_dir)\n",
    "print(f\"Open-TYNDP available in '{to_dir}'.\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68125e0502973f",
   "metadata": {},
   "source": [
    "# The `Snakemake` tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44c6959896526ad",
   "metadata": {},
   "source": [
    "![](snakemake_logo.png)\n",
    "\n",
    "The `Snakemake` workflow management system is a tool to create reproducible and scalable data analyses.\n",
    "Workflows are described via a human readable, Python based language. They can be seamlessly scaled to server, cluster, grid, and cloud environments, without the need to modify the workflow definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f61ff44",
   "metadata": {},
   "source": [
    "Snakemake follows the [GNU Make](https://www.gnu.org/software/make) paradigm: workflows are defined in terms of so-called `rules` that define how to create a set of output files from a set of input files. Dependencies between the rules are determined automatically, creating a DAG (directed acyclic graph) of jobs that can be automatically parallelized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c5c5e23a63e560",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "Documentation for this package is available at https://snakemake.readthedocs.io/. You can also check out a [slide deck Snakemake Tutorial](https://slides.com/johanneskoester/snakemake-tutorial) by Johannes Köster (2024).\n",
    "\n",
    "Mölder, F., Jablonski, K.P., Letcher, B., Hall, M.B., Tomkins-Tinch, C.H., Sochat, V., Forster, J., Lee, S., Twardziok, S.O., Kanitz, A., Wilm, A., Holtgrewe, M., Rahmann, S., Nahnsen, S., Köster, J., 2021. Sustainable data analysis with Snakemake. F1000Res 10, 33.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e82c9e22d6b8ce6",
   "metadata": {},
   "source": [
    "## A minimal Snakemake example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f936ab",
   "metadata": {},
   "source": [
    "To check out how this looks in practice, we've prepared a minimal Snakemake example workflow that processes some data. The minimal workflow consists of the following rules:\n",
    "- `retrieve_data`\n",
    "- `build_data`\n",
    "- `prepare_network`\n",
    "- `solve_network`\n",
    "- `plot_benchmark`\n",
    "- `all`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c33282d",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "![](minimal_workflow.png)\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534d9440",
   "metadata": {},
   "source": [
    "We will first need to load the raw data file used in this minimal example into our working directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458220db",
   "metadata": {},
   "source": [
    "### The `Snakefile` and `rules`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ef510f",
   "metadata": {},
   "source": [
    "The rules need to be defined in a so-called `Snakefile` that sits in your current working directory. For our minimal example the `Snakefile` looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dfe4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code(filename=\"Snakefile\", language=\"Python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677e4c85",
   "metadata": {},
   "source": [
    "### Calling a workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7ebade",
   "metadata": {},
   "source": [
    "You can then execute the workflow by specifying the target file `data/benchmark.pdf` or any intermediate file:\n",
    "```\n",
    "snakemake -call data/benchmark.pdf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9edee26",
   "metadata": {},
   "source": [
    "Alternatively you can also execute the workflow by calling a rule that produces an intermediate file:\n",
    "```\n",
    "snakemake -call build_data\n",
    "```\n",
    "**NOTE:** You cannot call a rule that includes a wildcard without specifying what the wildcard should be filled with. Otherwise Snakemake will not know what to propagate back."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6235bc82",
   "metadata": {},
   "source": [
    "Or you can call the common rule `all` which can be used to execute the entire workflow. It takes the final workflow output as its input and thus requires all previous dependent rules to be run as well:\n",
    "```\n",
    "snakemake -call all\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fd843e",
   "metadata": {},
   "source": [
    "A very important instrument is the `-n` flag which executes a `dry-run`. It is recommended to always first execute a `dry-run` before the actual execution of the workflow. This simply prints out the directed acyclic graph (DAG) of the workflow to investigate without actually executing it.\n",
    "\n",
    "Let's try this out and investigate the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec82288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! snakemake -call all -n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6ef812",
   "metadata": {},
   "source": [
    "### Visualizing the `DAG` of a workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff53df2",
   "metadata": {},
   "source": [
    "You can also visualize the `DAG` of jobs using the `--dag` flag and the Graphviz `dot` command. This will not run the workflow but only create the visualization:\n",
    "```\n",
    "snakemake -call all --dag | dot -Tsvg > dag.svg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e051153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! snakemake -call all --dag | sed -n \"/digraph/,\\$p\" | dot -Tpng > dag_minimal.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4af4831b0ac3b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T21:26:31.227226Z",
     "start_time": "2025-10-10T21:26:31.203248Z"
    }
   },
   "outputs": [],
   "source": [
    "display(Image(\"dag_minimal.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99494ce7",
   "metadata": {},
   "source": [
    "Alternatively, you can also visualize a filegraph like the figure above which also includes some information about the inputs and outputs to each of the rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d70368",
   "metadata": {},
   "source": [
    "You can reproduce the figure from above with the following command:\n",
    "```\n",
    "snakemake -call all --filegraph | dot -Tsvg > filegraph.svg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec22d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! snakemake -call all --filegraph | sed -n \"/digraph/,\\$p\" | dot -Tsvg > filegraph_minimal.svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c99998a2f27173",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(SVG(\"filegraph_minimal.svg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc74c230",
   "metadata": {},
   "source": [
    "## Task 1: Executing a workflow with Snakemake\n",
    "\n",
    "a) For our minimal example, execute a `dry-run` to produce the intermediate file `data/base_2030.nc`.\n",
    "\n",
    "b) Execute the workflow and investigate what happens if you try to execute the workflow again.\n",
    "\n",
    "c) Delete the final output files `data/benchmark.pdf` and investigate what happens if you try to execute the workflow again.\n",
    "\n",
    "d) Import the raw input data file `data/data_raw.csv` using pandas and save it again overwriting the original file. Investigate what happens if you try to execute the workflow again. <br>\n",
    "Hint: Alternatively you can also just `touch` the file by executing `from pathlib import Path` and `Path(\"data/data_raw.csv\").touch()`\n",
    "\n",
    "e) Finally, open the `Snakefile` and add a second rule that processes the file `data_raw_2.csv` using the same script as the `build_data` rule. Add the output of this new rule as a second input to the `prepare_network` rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fc1a64ecb68ac3",
   "metadata": {},
   "source": [
    "## Using Snakemake to launch the open-TYNDP workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759df1ccb8f8f179",
   "metadata": {},
   "source": [
    "We have already retrieved a prebuilt version of the `open-tyndp` GitHub repository into our working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b5b10c",
   "metadata": {},
   "source": [
    "The `open-tyndp` repository contains the following structure. Directories of particular interest are marked in bold:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea24d976",
   "metadata": {},
   "source": [
    "- **benchmarks**: will store snakemake benchmarks (does not exist initially)\n",
    "- **config**: configurations used in the study\n",
    "- cutouts: will store raw weather data cutouts from atlite (does not exist initially)\n",
    "- **data**: includes input data that is not produced by any snakemake rule. Various different input files are retrieved from external storage and stored in this directory\n",
    "- doc: includes all files necessary to build the readthedocs documentation of PyPSA-Eur\n",
    "- **envs**: includes all the mamba environment specifications to run the workflow\n",
    "- logs: will store log files (does not exist initially)\n",
    "- **notebooks**: includes all the notebooks used for ad-hoc analysis\n",
    "- report: contains all files necessary to build the report; plots and result files are generated automatically\n",
    "- **rules**: includes all the snakemake rules loaded in the Snakefile\n",
    "- **resources**: will store intermediate results of the workflow which can be picked up again by subsequent rules (does not exist initially)\n",
    "- **results**: will store the solved PyPSA network data, summary files and output plots (does not exist initially)\n",
    "- **scripts**: includes all the Python scripts executed by the snakemake rules to build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bc2c6b",
   "metadata": {},
   "source": [
    "## Task 2: Explore the folder\n",
    "\n",
    "a) Can you find the TYNDP specific data input files?\n",
    "\n",
    "b) Where can you verify the scenario and planning horizons used to produce the current results?\n",
    "\n",
    "(hint: search for `config.tyndp.yaml`)\n",
    "\n",
    "c) Can you find the hydrogen grid map in the outputs files for the NT scenario in 2040?\n",
    "\n",
    "(hint: search for `base_s_all__-h2_network_2040.pdf`)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29b1200c02ce786",
   "metadata": {},
   "source": [
    "We now need to change our working directory to this new directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f190d685184837b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"data/open-tyndp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e39aa4",
   "metadata": {},
   "source": [
    "Let's check that we are indeed in the new directory now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db99e626b8b0309",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7e3e50",
   "metadata": {},
   "source": [
    "We can now use Snakemake to call some of the rules to produce outputs with the `open-tyndp` PyPSA model. \n",
    "\n",
    "We will use the prepared TYNDP configuration file (`config/config.tyndp.yaml`) and schedule a dry-run with `-n` as we only want to investigate the DAG of the workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a547c6f4a2d82607",
   "metadata": {},
   "outputs": [],
   "source": [
    "! snakemake -call all --configfile config/config.tyndp.yaml -n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561132a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Touch files to show snakemake logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9208a03",
   "metadata": {},
   "source": [
    "The corresponding rule graph to this workflow will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f4913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! snakemake -call all -F --rulegraph | sed -n \"/digraph/,\\$p\" | dot -Tpng > rulegraph_open_tyndp.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4749ee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(\"rulegraph_open_tyndp.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d009cc9b",
   "metadata": {},
   "source": [
    "As you can see, this workflow is much more complicated than our minimal example from the beginning.\n",
    "\n",
    "However, the general idea remains the same. We retrieve data which we consequently process, then we prepare the model network and we solve it before we postprocess the results (summary, plotting, benchmarks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4aaf85",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "If you are executing this notebook on your local machine, you can also use the `conda` package manager to install the `open-tyndp` environment and run the workflow instead of dry-runs:\n",
    "```\n",
    "conda env create --file envs/<YourSystemOS>-pinned.yaml\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0bcb21af782ebd",
   "metadata": {},
   "source": [
    "## Task 3: Adjusting the Open-TYNDP workflow with the configuration file\n",
    "\n",
    "a) Make some changes in the configuration file and call another **dry-run** of the `open-tyndp` model again to see the changes to the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753af7b6",
   "metadata": {},
   "source": [
    "Disabling all sectors, for instance, reduces the number of rules to be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451d4d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7930fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! snakemake -call --configfile config/config.tyndp.yaml -n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f5e7d",
   "metadata": {},
   "source": [
    "# Update on new features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d279af",
   "metadata": {},
   "source": [
    "For the purposes of this workshop, we will primarily focus on the National Trends (NT) scenario when applicable. Four major features were introduced since our last workshop:\n",
    "1. Addition of the electricity demand and PECD capacity factors time series,\n",
    "2. Addition of onshore wind and solar TYNDP technologies (incl. PEMMDB existing capacities and trajectories),\n",
    "3. Addition of offshore hubs (incl. the offshore topology, all associated technologies, potential constraints and trajectories),\n",
    "4. Addition of hydrogen import corridors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97193552",
   "metadata": {},
   "source": [
    "# Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b8565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for renewables components\n",
    "# Explain / remind difference between time varying and fixed attributes, how to access them\n",
    "# Plot both time of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60f0668",
   "metadata": {},
   "source": [
    "The Open-TYNDP data we retrieved contains a network with low time resolution. This is illustrative; however, since we are focusing on time series, we will use another higher resolution network. So, we will import a high resolution pre-solved network for the National Trends (NT) scenario for 2030."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2079eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hourly networks\n",
    "n_NT_2030h = pypsa.Network(\"../network_NT_presolve_highres_2030.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3333d214",
   "metadata": {},
   "source": [
    "## Electricity demand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b52e94",
   "metadata": {},
   "source": [
    "We can then explore the electricity demand that is attached to the network. Can you remember how to access `Loads` time series in PyPSA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46775be5",
   "metadata": {},
   "source": [
    "Correct! You can use the `loads_t` key and its `p_set` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d3ae39",
   "metadata": {},
   "outputs": [],
   "source": [
    "loads_2030 = n_NT_2030h.loads_t.p_set\n",
    "loads_2030.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05458163",
   "metadata": {},
   "source": [
    "Let's plot the electricity demand time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85992bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "loads_2030.div(1e3).plot(\n",
    "    xlabel=\"Time\",\n",
    "    ylabel=\"Load [GW]\",\n",
    "    title=\"Electricity Load Time Series - NT - 2030\",\n",
    "    grid=True,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "ax.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.15), ncols=10)\n",
    "ax.grid(True, linestyle=\"--\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd854e",
   "metadata": {},
   "source": [
    "This is very confusing to look at. Let's filter that down a bit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bfe14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group country profiles together and select a week\n",
    "country_mapping = n_NT_2030h.buses.query(\"carrier=='AC'\").country\n",
    "loads_2030_by_country = (\n",
    "    n_NT_2030h.loads_t.p_set.T.rename(country_mapping, axis=0)\n",
    "    .groupby(\"Load\")\n",
    "    .sum()\n",
    "    .T.loc[\"2009-03-01\":\"2009-03-07\", [\"FR\", \"DE\", \"GB\"]]\n",
    ")\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "loads_2030_by_country.div(1e3).plot(\n",
    "    xlabel=\"Time\",\n",
    "    ylabel=\"Load [GW]\",\n",
    "    title=\"Electricity Load Time Series - NT - 2030\",\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "ax.grid(True, linestyle=\"--\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e32485",
   "metadata": {},
   "source": [
    "## Task 4: Use PyPSA statistics to explore the load\n",
    "\n",
    "Can you remember how to use the **PyPSA Statistics** module that we introduced in the last workshop to interactively visualize these electricity demand inputs from the network:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7672932",
   "metadata": {},
   "source": [
    "First, we need to load a lower-resolution network that was solved, so we can use the statistics module to analyze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7181673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_NT_2030 = pypsa.Network(\"results/tyndp/NT/networks/base_s_all___2030.nc\")\n",
    "n_NT_2030.carriers.loc[\"none\", \"color\"] = \"#000000\"\n",
    "n_NT_2030.carriers.loc[\"\", \"color\"] = \"#000000\"\n",
    "\n",
    "n_DE_2040 = pypsa.Network(\"results/tyndp/DE/networks/base_s_all___2040.nc\")\n",
    "n_DE_2040.carriers.loc[\"none\", \"color\"] = \"#000000\"\n",
    "n_DE_2040.carriers.loc[\"\", \"color\"] = \"#000000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b9c885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define a helper variable\n",
    "s = n_NT_2030.statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d1658",
   "metadata": {},
   "source": [
    "Let's access the data using `s.withdrawal()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4952e155",
   "metadata": {},
   "outputs": [],
   "source": [
    "s.withdrawal(\n",
    "    bus_carrier=\"low voltage\", comps=\"Load\", aggregate_time=False, groupby=False\n",
    ").T.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41247bcd",
   "metadata": {},
   "source": [
    "We can also plot all the countries at the same time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957d953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, facet_grid = s.withdrawal.plot.line(\n",
    "    bus_carrier=\"low voltage\",\n",
    "    y=\"value\",\n",
    "    x=\"snapshot\",\n",
    "    color=\"country\",\n",
    ")\n",
    "fig.suptitle(\"Electricity demand Time Series - NT - 2030\", y=1.05)\n",
    "ax.set_ylabel(\"Load [MW]\")\n",
    "ax.set_xlabel(\"Time\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d173ee",
   "metadata": {},
   "source": [
    "This is once again difficult to read. Let's keep only two countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a4a6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, facet_col = s.withdrawal.plot.area(\n",
    "    bus_carrier=\"low voltage\",\n",
    "    y=\"value\",\n",
    "    x=\"snapshot\",\n",
    "    color=\"carrier\",\n",
    "    stacked=True,\n",
    "    facet_row=\"country\",\n",
    "    query=\"carrier == 'electricity' and country in ['DE', 'FR']\",\n",
    ")\n",
    "fig.suptitle(\"Electricity demand Time Series - NT - 2030\", y=1.05)\n",
    "ax[0, 0].set_ylabel(\"Load [MW]\")\n",
    "ax[1, 0].set_ylabel(\"Load [MW]\")\n",
    "ax[1, 0].set_xlabel(\"Time\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93a51e6",
   "metadata": {},
   "source": [
    "## PECD capacity factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acd585d",
   "metadata": {},
   "source": [
    "The Pan-European Climate Database (PECD) provides capacity factor profiles for all the different renewable technologies used in the TYNDP. We processed these input data files into a Python and PyPSA friendly input format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea2437f",
   "metadata": {},
   "source": [
    "Let's start by looking at the processed capacity factor time series for solar PV Utility for 2030. These processed data are stored in the `resources` directory, as they are an output of `build_renewable_profiles_pecd`. We will filter the data to a set of countries and a week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db551850",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_pv_rftp = pd.read_csv(\n",
    "    \"resources/tyndp/NT/pecd_data_LFSolarPVRooftop_2030.csv\",\n",
    "    index_col=0,\n",
    "    parse_dates=True,\n",
    ").loc[\"2009-07-01\":\"2009-07-04\", [\"SE04\", \"DE00\", \"FR00\", \"ES00\"]]\n",
    "cf_pv_rftp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c853fb2",
   "metadata": {},
   "source": [
    "Using a heatmap, we can better grasp the content of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0947d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.heatmap(cf_pv_rftp.T, cmap=\"viridis\", cbar_kws={\"label\": \"Capacity Factor\"}, ax=ax)\n",
    "\n",
    "tick_positions = range(0, len(cf_pv_rftp), 24)\n",
    "ax.set_xticks(tick_positions)\n",
    "ax.set_xticklabels(\n",
    "    cf_pv_rftp.index[tick_positions].strftime(\"%Y-%m-%d\"), rotation=45, ha=\"right\"\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"Node\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73476ae",
   "metadata": {},
   "source": [
    "We can also present the data as time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4c327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "cf_pv_rftp.plot(\n",
    "    title=\"Capacity Factor Time Series National Trends 2030 - March 1-4\",\n",
    "    xlabel=\"Date\",\n",
    "    ylabel=\"Capacity Factor\",\n",
    "    ax=ax,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7785fc1f",
   "metadata": {},
   "source": [
    "## Task 5: Compute average capacity factor\n",
    "\n",
    "a) Locate the resource file with onwind capacity factors used for NT in 2030.\n",
    "\n",
    "b) Compute the average onwind capacity factor for all the countries in the PECD.\n",
    "\n",
    "c) Verify one of the values directly in the network.\n",
    "\n",
    "(hint: capacity factors are defined as time varying parameter of generators and are called `p_max_pu`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a9d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_onwind = pd.read_csv(\n",
    "    \"resources/tyndp/NT/pecd_data_Wind_Onshore_2030.csv\",\n",
    "    index_col=0,\n",
    "    parse_dates=True,\n",
    ")\n",
    "cf_onwind.head();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a498d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_onwind.columns = cf_onwind.columns.str[:2]\n",
    "cf_onwind.columns.name = \"country\"\n",
    "cf_onwind.head();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b3880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_onwind_sorted = cf_onwind.T.groupby(by=\"country\").mean().mean(axis=1).sort_values()\n",
    "cf_onwind_sorted.tail();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a78cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 9))\n",
    "cf_onwind_sorted.plot.barh(\n",
    "    title=\"Average capacity factors - 2030\",\n",
    "    xlabel=\"Capacity factor [p.u]\",\n",
    "    ylabel=\"Country\",\n",
    "    ax=ax,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1e4d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_buses = n_NT_2030h.buses.query(\"country=='IE'\").index\n",
    "c_gen = n_NT_2030h.generators.query(\"carrier=='onwind' and bus in @c_buses\").index\n",
    "c_cf = n_NT_2030h.generators_t.p_max_pu[c_gen]\n",
    "c_cf.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad8ea68",
   "metadata": {},
   "source": [
    "# Onshore wind and solar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4066e",
   "metadata": {},
   "source": [
    "The TYNDP provides expansion trajectories for given investment candidates and expandable technologies. For the implemented onshore wind and solar technologies, these have been included in beta release v0.3 of the Open-TYNDP model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4d750f",
   "metadata": {},
   "source": [
    "It is possible to retrieve those values from the network. However, for simplicity, we will import the values directly from the processed input files for the `DE` scenario to investigate the entire trajectory paths at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a651700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = pd.read_csv(\"resources/tyndp/DE/tyndp_trajectories.csv\")\n",
    "trajectories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cbceb0",
   "metadata": {},
   "source": [
    "Similar to the capacity factor time series, we want to focus on the Solar PV Rooftop technology and its trajectory path. Let's take Germany (DE00) to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b984f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories_pv_utility_de = (\n",
    "    trajectories.query(\"carrier == 'solar-pv-rooftop' and bus == 'DE00'\")\n",
    "    .sort_values(by=\"pyear\")\n",
    "    .set_index(\"pyear\")[[\"p_nom_min\", \"p_nom_max\"]]\n",
    "    .div(1e3)  # GW\n",
    ")\n",
    "trajectories_pv_utility_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8e1844",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "trajectories_pv_utility_de.plot(\n",
    "    title=\"Solar PV Utility Capacity Trajectories - DE scenario - DE00\",\n",
    "    xlabel=\"Planning Year\",\n",
    "    ylabel=\"Capacity [GW]\",\n",
    "    color=[\"#E63946\", \"#1D3557\"],\n",
    "    style=\"*--\",\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "ax.fill_between(\n",
    "    trajectories_pv_utility_de.index,\n",
    "    trajectories_pv_utility_de.iloc[:, 0],\n",
    "    trajectories_pv_utility_de.iloc[:, 1],\n",
    "    alpha=0.25,\n",
    "    color=\"#457B9D\",\n",
    "    label=\"Trajectory Range\",\n",
    ")\n",
    "\n",
    "ax.xaxis.set_major_locator(MultipleLocator(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d957d3c",
   "metadata": {},
   "source": [
    "Now, let's access the network for the DE scenario to compare one of these trajectory values for 2040."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567331da",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories_pv_utility_de_from_network = n_DE_2040.generators.query(\n",
    "    \"carrier == 'solar-pv-utility' and bus == 'DE00'\"\n",
    ")[[\"p_nom\", \"p_nom_min\", \"p_nom_max\"]].div(\n",
    "    1e3\n",
    ")  # in GW\n",
    "trajectories_pv_utility_de_from_network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a3c693",
   "metadata": {},
   "source": [
    "As we can see, the `p_nom_max` and `p_nom_min` values for 2040 do not match directly with the reported trajectory values analyzed above. This is because each new Generator will have set trajectories that correspond to the new cumulatively installed capacities taking into account optimization results from previous years. Therefore, if we add up the existing capacity (`p_nom`) from 2030 to the `p_nom_max` and `p_nom_min` values from 2040, we will obtain the reported trajectory values shown above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0899d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    trajectories_pv_utility_de_from_network.loc[\n",
    "        \"DE00 0 solar-pv-utility-2040\", [\"p_nom_min\", \"p_nom_max\"]\n",
    "    ]\n",
    "    + trajectories_pv_utility_de_from_network.loc[\n",
    "        \"DE00 0 solar-pv-utility-2030\", \"p_nom\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f0d930",
   "metadata": {},
   "source": [
    "## Task 6: Verify onshore wind trajectories\n",
    "\n",
    "Verify onshore wind trajectories in the network itself. This can be quick if you can copy and reuse the existing code used above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5d366c",
   "metadata": {},
   "source": [
    "# Offshore Hubs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705131c2",
   "metadata": {},
   "source": [
    "To implement the offshore methodology, new carriers (*i.e.*, technologies) are introduced. All the offshore technologies start with `offwind`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d46b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "offwind_carriers = n_NT_2030.carriers.query(\"Carrier.str.contains('offwind')\")\n",
    "offwind_carriers_i = offwind_carriers.index\n",
    "offwind_carriers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585d7896",
   "metadata": {},
   "source": [
    "As you can see in the table above, all the offshore technologies are implemented. We model technologies that are a combination of the following:\n",
    "- both AC `ac` and DC `dc` zones, as well as H2 generating windfarms `h2`;\n",
    "- both fixed-bottom `fb` and floating `fl` foundations;\n",
    "- both radial `r` and hub `oh` connections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf4935d",
   "metadata": {},
   "source": [
    "We also introduce new offshore buses, both for electricity and hydrogen. Electricity buses use `AC_OH` as the carrier, while hydrogen buses use `H2_OH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1ea60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_NT_2030.buses.query(\"carrier.str.contains('OH')\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940bb32a",
   "metadata": {},
   "source": [
    "Let's narrow down this list to a single country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd71d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "buses = n_NT_2030.buses.query(\"carrier.str.contains('OH') and country=='BE'\")\n",
    "buses_i = buses.index\n",
    "buses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b3577",
   "metadata": {},
   "source": [
    "Using `n.plot.explore()`, we can easily get an overview of the network topology. Let's clean the network before exploring it to only focus on the electrical offshore topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb486c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's filter AC_OH buses and explore\n",
    "n_explore_ac = n_NT_2030.copy()\n",
    "n_explore_ac.remove(\"Bus\", n_explore_ac.buses.query(\"carrier not in ['AC_OH']\").index)\n",
    "n_explore_ac.plot.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9a521b",
   "metadata": {},
   "source": [
    "Now, we can search the network for generators that are defined with those carriers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07229e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_NT_2030.generators.query(\"carrier in @offwind_carriers_i\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b187e",
   "metadata": {},
   "source": [
    "Let's focus on a specific country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2860601",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_NT_2030.generators.query(\"carrier in @offwind_carriers_i and bus in @buses_i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07ca985",
   "metadata": {},
   "source": [
    "## Task 7: Extract existing offshore capacities\n",
    "\n",
    "Extract existing offshore capacities for the country of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47bb4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_DE_2040.statistics.optimal_capacity(\n",
    "    bus_carrier=[\"AC\", \"AC_OH\", \"H2_OH\"],\n",
    "    comps=\"Generator\",\n",
    "    groupby=[\"bus\"],\n",
    ").to_frame().query(\"bus.str.contains('BE')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b34b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, facet_grid = n_DE_2040.statistics.optimal_capacity.plot.bar(\n",
    "    bus_carrier=[\"AC\", \"AC_OH\", \"H2_OH\"],\n",
    "    query=\"carrier.str.startswith('offwind') and country in ['NL', 'GB']\",\n",
    "    facet_col=\"country\",\n",
    ")\n",
    "fig.suptitle(\"Offshore wind capacities - DE - 2040\", y=1.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa0d42f",
   "metadata": {},
   "source": [
    "We can also explore the data with maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7309445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clean a network copy to only keep offshore data\n",
    "n_map = n_DE_2040.copy()\n",
    "n_map.remove(\"Bus\", n_map.buses.query(\"carrier not in ['AC', 'AC_OH', 'H2_OH']\").index)\n",
    "n_map.remove(\n",
    "    \"Generator\", n_map.generators.query(\"not carrier.str.startswith('offwind')\").index\n",
    ")\n",
    "n_map.remove(\"Link\", n_map.links.index)\n",
    "n_map.remove(\"StorageUnit\", n_map.storage_units.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03488719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define map projection\n",
    "def load_projection(plotting_params):\n",
    "    proj_kwargs = plotting_params.get(\"projection\", dict(name=\"EqualEarth\"))\n",
    "    proj_func = getattr(ccrs, proj_kwargs.pop(\"name\"))\n",
    "    return proj_func(**proj_kwargs)\n",
    "\n",
    "\n",
    "proj = load_projection(dict(name=\"EqualEarth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d304aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the map\n",
    "subplot_kw = {\"projection\": proj}\n",
    "fig, ax = plt.subplots(figsize=(12, 12), subplot_kw=subplot_kw)\n",
    "n_map.statistics.optimal_capacity.plot.map(\n",
    "    bus_carrier=[\"AC\", \"AC_OH\", \"H2_OH\"],\n",
    "    ax=ax,\n",
    "    bus_area_fraction=0.002,\n",
    "    title=\"Offshore wind capacities - DE - 2040\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82157b9",
   "metadata": {},
   "source": [
    "The NT scenario is a dispatch scenario. This is translated in PyPSA with the argument `p_nom_extendable = False`. However, for the two other scenario, we need to model capacity expansion. \n",
    "\n",
    "Currently, the model is configured to do myopic optimisaiton. This means that only the capacities of the current planning horizon are expandable. Generators of the previous planning horizon are not. Let's check this the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04fa77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore offshore wind generators in Danemark\n",
    "c_buses = n_DE_2040.buses.query(\"country == 'DK'\").index\n",
    "(\n",
    "    n_DE_2040.generators.query(\"carrier in @offwind_carriers_i and bus in @c_buses\")[\n",
    "        [\"p_nom\", \"p_nom_min\", \"p_nom_max\", \"p_nom_opt\", \"p_nom_extendable\"]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf4a2e0",
   "metadata": {},
   "source": [
    "As you can in the table, multiple values exists:\n",
    "- `p_nom`, the nominal power\n",
    "- `p_nom_min`, if p_nom is extendable, the minimal value\n",
    "- `p_nom_max`, if p_nom is extendable, the maximal value\n",
    "- `p_nom_opt`, the optimised nominal power\n",
    "\n",
    "The `p_nom_min` reflects the exesting capacities defined in the TYNDP, while the `p_nom_max` represent the layer potential. We also implemented constraints to ensure to respect the zone potentials and the trajectories defined in the data:\n",
    "- A constraint limits the expansion of DC and H2 sitting on the same location, as the sum of the two capacities cannot exceed the layer potential.\n",
    "- A constraint sets the maximum potential per zone, taking into account the zone trajectories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee8acb4",
   "metadata": {},
   "source": [
    "# H2 imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c32b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read network\n",
    "# plot H2 map with import corridors\n",
    "# table to showcase the corridors\n",
    "# optional: Task to investigate values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bac7c7f",
   "metadata": {},
   "source": [
    "There have also been some important additions to the H2 infrastructure since our last workshop. The different H2 import corridors are now included in the model with a simple pipeline transport representation, similar to the H2 reference grid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a5ac92",
   "metadata": {},
   "source": [
    "We can investigate our National Trends network to create a similar plot to what we showed last time. Let's define some (quite extensive) plotting functions from the open-tyndp workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e5d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_import_corridors(df):\n",
    "    \"\"\"\n",
    "    Group pipes which connect same buses and return overall capacity.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # there are pipes for each investment period rename to AC buses name for plotting\n",
    "    df[\"index_orig\"] = df.index\n",
    "    df.rename(index=lambda x: x.split(\" - \")[0], inplace=True)\n",
    "    return df.groupby(level=0).agg(\n",
    "        {\"p_nom\": \"sum\", \"p_nom_opt\": \"sum\", \"index_orig\": \"first\"}\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_h2_map_base(\n",
    "    network,\n",
    "    map_opts,\n",
    "    proj,\n",
    "    figsize=(12, 12),\n",
    "    expanded=False,\n",
    "    regions_for_storage=None,\n",
    "    color_h2_pipe=\"#499a9c\",\n",
    "    color_h2_imports=\"#FFA500\",\n",
    "    color_h2_node=\"#ff29d9\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots the base hydrogen network pipelines capacities, hydrogen buses and import potentials.\n",
    "    If expanded is enabled, the optimal capacities are plotted instead.\n",
    "    If regions are given, hydrogen storage capacities are plotted for those regions with aggregated H2 tank storage\n",
    "    and underground H2 cavern capacities.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network : pypsa.Network\n",
    "        PyPSA network for plotting the hydrogen grid. Can be either presolving or post solving.\n",
    "    map_opts : dict\n",
    "        Map options for plotting.\n",
    "    expanded : bool, optional\n",
    "        Whether to plot expanded capacities. Defaults to plotting only base network (p_nom).\n",
    "    regions_for_storage : gpd.GeoDataframe, optional\n",
    "        Geodataframe of regions to use for plotting hydrogen storage capacities. Index needs to match storage locations.\n",
    "        If none is given, no hydrogen storage capacities are plotted.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Saves the map plot as figure.\n",
    "    \"\"\"\n",
    "    n = network.copy()\n",
    "\n",
    "    linewidth_factor = 4e3\n",
    "\n",
    "    n.links.drop(\n",
    "        n.links.index[\n",
    "            ~(\n",
    "                n.links.carrier.str.contains(\"H2 pipeline\")\n",
    "                | n.links.carrier.str.contains(\"H2 import\")\n",
    "            )\n",
    "        ],\n",
    "        inplace=True,\n",
    "    )\n",
    "    n.links.drop(\n",
    "        n.links.index[n.links.carrier.str.contains(\"OH\")],\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    p_nom = \"p_nom_opt\" if expanded else \"p_nom\"\n",
    "    # capacity of pipes and imports\n",
    "    h2_pipes = n.links[n.links.carrier == \"H2 pipeline\"][p_nom]\n",
    "    h2_imports = n.links[n.links.carrier.str.contains(\"H2 import\")]\n",
    "\n",
    "    # group high and low import corridors together\n",
    "    h2_imports = group_import_corridors(h2_imports)[p_nom]\n",
    "    n.links.rename(index=lambda x: x.split(\" - \")[0], inplace=True)\n",
    "    # group links by summing up p_nom values and taking the first value of the rest of the columns\n",
    "    other_cols = dict.fromkeys(n.links.columns.drop([\"p_nom_opt\", \"p_nom\"]), \"first\")\n",
    "    n.links = n.links.groupby(level=0).agg(\n",
    "        {\"p_nom_opt\": \"sum\", \"p_nom\": \"sum\", **other_cols}\n",
    "    )\n",
    "\n",
    "    # set link widths\n",
    "    link_widths_pipes = h2_pipes / linewidth_factor\n",
    "    link_widths_imports = h2_imports / linewidth_factor\n",
    "    if link_widths_pipes.notnull().empty:\n",
    "        print(\"No base H2 pipeline capacities to plot.\")\n",
    "        return\n",
    "    link_widths_pipes = link_widths_pipes.reindex(n.links.index).fillna(0.0)\n",
    "    link_widths_imports = link_widths_imports.reindex(n.links.index).fillna(0.0)\n",
    "\n",
    "    # drop non H2 buses\n",
    "    n.buses.drop(\n",
    "        n.buses.index[\n",
    "            (~n.buses.carrier.str.contains(\"H2\")) | (n.buses.carrier.str.contains(\"OH\"))\n",
    "        ],\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # optionally add hydrogen storage capacities onto the map\n",
    "    if regions_for_storage is not None:\n",
    "        h2_storage = n.stores.query(\"carrier.str.contains('H2')\")\n",
    "        regions_for_storage[\"H2\"] = (\n",
    "            h2_storage.rename(index=h2_storage.bus.map(n.buses.location))\n",
    "            .e_nom_opt.groupby(level=0)\n",
    "            .sum()\n",
    "            .div(1e6)\n",
    "        )  # TWh\n",
    "        regions_for_storage[\"H2\"] = regions_for_storage[\"H2\"].where(\n",
    "            regions_for_storage[\"H2\"] > 0.1\n",
    "        )\n",
    "\n",
    "    # plot H2 pipeline capacities and imports\n",
    "    print(\"Plotting base H2 pipeline and import capacities.\")\n",
    "    fig, ax = plt.subplots(figsize=figsize, subplot_kw={\"projection\": proj})\n",
    "\n",
    "    n.plot.map(\n",
    "        geomap=True,\n",
    "        bus_sizes=0.1,\n",
    "        bus_colors=color_h2_node,\n",
    "        link_colors=color_h2_pipe,\n",
    "        link_widths=link_widths_pipes,\n",
    "        branch_components=[\"Link\"],\n",
    "        ax=ax,\n",
    "        **map_opts,\n",
    "    )\n",
    "\n",
    "    if regions_for_storage is not None:\n",
    "        regions_for_storage = regions_for_storage.to_crs(proj.proj4_init)\n",
    "        regions_for_storage.plot(\n",
    "            ax=ax,\n",
    "            column=\"H2\",\n",
    "            cmap=\"Blues\",\n",
    "            linewidths=0,\n",
    "            legend=True,\n",
    "            vmax=6,\n",
    "            vmin=0,\n",
    "            legend_kwds={\n",
    "                \"label\": \"Hydrogen Storage [TWh]\",\n",
    "                \"shrink\": 0.7,\n",
    "                \"extend\": \"max\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "    if not h2_imports.empty:\n",
    "        n.plot.map(\n",
    "            geomap=True,\n",
    "            bus_sizes=0,\n",
    "            link_colors=color_h2_imports,\n",
    "            link_widths=link_widths_imports,\n",
    "            branch_components=[\"Link\"],\n",
    "            ax=ax,\n",
    "            **map_opts,\n",
    "        )\n",
    "\n",
    "    sizes = [30, 10]\n",
    "    labels = [f\"{s} GW\" for s in sizes]\n",
    "    scale = 1e3 / linewidth_factor\n",
    "    sizes = [s * scale for s in sizes]\n",
    "\n",
    "    legend_kw = dict(\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(0.32, 1.13),\n",
    "        frameon=False,\n",
    "        ncol=1,\n",
    "        labelspacing=0.8,\n",
    "        handletextpad=1,\n",
    "    )\n",
    "\n",
    "    add_legend_lines(\n",
    "        ax,\n",
    "        sizes,\n",
    "        labels,\n",
    "        patch_kw=dict(color=\"lightgrey\"),\n",
    "        legend_kw=legend_kw,\n",
    "    )\n",
    "\n",
    "    legend_kw = dict(\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(0.55, 1.13),\n",
    "        labelspacing=0.8,\n",
    "        handletextpad=0,\n",
    "        frameon=False,\n",
    "    )\n",
    "\n",
    "    add_legend_circles(\n",
    "        ax,\n",
    "        sizes=[0.2],\n",
    "        labels=[\"H2 Node\"],\n",
    "        srid=n.srid,\n",
    "        patch_kw=dict(facecolor=color_h2_node),\n",
    "        legend_kw=legend_kw,\n",
    "    )\n",
    "\n",
    "    colors = (\n",
    "        [color_h2_pipe, color_h2_imports] if not h2_imports.empty else [color_h2_pipe]\n",
    "    )\n",
    "    labels = [\"H2 Pipeline\", \"H2 import\"] if not h2_imports.empty else [\"H2 Pipeline\"]\n",
    "\n",
    "    legend_kw = dict(\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(0, 1.13),\n",
    "        ncol=1,\n",
    "        frameon=False,\n",
    "    )\n",
    "\n",
    "    add_legend_patches(ax, colors, labels, legend_kw=legend_kw)\n",
    "\n",
    "    ax.set_facecolor(\"white\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3ec517",
   "metadata": {},
   "source": [
    "And plot the H2 reference grid together with the import corridors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8020a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = n_NT_2030.copy()\n",
    "\n",
    "\n",
    "def load_projection(plotting_params):\n",
    "    proj_kwargs = plotting_params.get(\"projection\", dict(name=\"EqualEarth\"))\n",
    "    proj_func = getattr(ccrs, proj_kwargs.pop(\"name\"))\n",
    "    return proj_func(**proj_kwargs)\n",
    "\n",
    "\n",
    "proj = load_projection(dict(name=\"EqualEarth\"))\n",
    "\n",
    "map_opts = {\n",
    "    \"boundaries\": [-11, 30, 34, 71],\n",
    "    \"geomap_colors\": {\n",
    "        \"ocean\": \"white\",\n",
    "        \"land\": \"white\",\n",
    "    },\n",
    "}\n",
    "\n",
    "if n.buses.country.isin([\"MA\", \"DZ\"]).any():\n",
    "    map_opts[\"boundaries\"] = list(np.add(map_opts[\"boundaries\"], [0, 0, -6, 0]))\n",
    "\n",
    "plot_h2_map_base(n, map_opts, proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16ae2fe",
   "metadata": {},
   "source": [
    "# Benchmarking framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e62f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Present metrics used (incl. reference to methodology)\n",
    "# Data sources used for comparison\n",
    "# Mention introduction of onwind and solar\n",
    "# Showcase current status\n",
    "# --- Table\n",
    "# --- Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6f22d8",
   "metadata": {},
   "source": [
    "# Wrap up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da40d5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect feedback via Slido"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
